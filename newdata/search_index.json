[["機器學習消費者投訴分類系統-1.html", "Chapter 4 機器學習：消費者投訴分類系統 4.1 Step 1. Data preparation for machine learning 4.2 Step 2. Specifying step elements for Recipe 4.3 Step 3. Build a workflow for Machine Learning 4.4 Step 4. K-Fold Cross-Validation 4.5 Step 5. Models Selection", " Chapter 4 機器學習：消費者投訴分類系統 核心目標：建立一個自動化的文本分類系統，以機器學習的方式，分析消費者的投訴之文字描述，預測其是否屬於「信用相關 (credit) 」或「其他產品 (other)」類別。 子目標： 開發一個準確的機器學習模型，結合數據預處理和模型訓練，實現分類任務的自動化。 使用斷詞、正則法、TF-IDF提取文本特徵。探索分類模型（如朴素貝葉斯、LASSO），比較模型的性能，並挑選最佳模型進行運用。 評估模型性能，包括準確率、ROC AUC、混淆矩陣等，確保模型能穩定應用。 最終目標：為企業提供自動化的文本分類工具，節省人工處理投訴的時間和成本，提升客戶服務的效率。 4.1 Step 1. Data preparation for machine learning 導入文本資料，共有18個欄位，117214筆資料。 complaints &lt;- readr::read_csv(&quot;/Users/sharon728/Downloads/complaints.csv.gz&quot;) ## Rows: 117214 Columns: 18 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (15): product, sub_product, issue, sub_issue, consumer_complaint_narrat... ## dbl (1): complaint_id ## date (2): date_received, date_sent_to_company ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(complaints) ## Rows: 117,214 ## Columns: 18 ## $ date_received &lt;date&gt; 2019-09-24, 2019-10-25, 2019-11-08, 2019… ## $ product &lt;chr&gt; &quot;Debt collection&quot;, &quot;Credit reporting, cre… ## $ sub_product &lt;chr&gt; &quot;I do not know&quot;, &quot;Credit reporting&quot;, &quot;I d… ## $ issue &lt;chr&gt; &quot;Attempts to collect debt not owed&quot;, &quot;Inc… ## $ sub_issue &lt;chr&gt; &quot;Debt is not yours&quot;, &quot;Information belongs… ## $ consumer_complaint_narrative &lt;chr&gt; &quot;transworld systems inc. \\nis trying to c… ## $ company_public_response &lt;chr&gt; NA, &quot;Company has responded to the consume… ## $ company &lt;chr&gt; &quot;TRANSWORLD SYSTEMS INC&quot;, &quot;TRANSUNION INT… ## $ state &lt;chr&gt; &quot;FL&quot;, &quot;CA&quot;, &quot;NC&quot;, &quot;RI&quot;, &quot;FL&quot;, &quot;TX&quot;, &quot;SC&quot;,… ## $ zip_code &lt;chr&gt; &quot;335XX&quot;, &quot;937XX&quot;, &quot;275XX&quot;, &quot;029XX&quot;, &quot;333X… ## $ tags &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ consumer_consent_provided &lt;chr&gt; &quot;Consent provided&quot;, &quot;Consent provided&quot;, &quot;… ## $ submitted_via &lt;chr&gt; &quot;Web&quot;, &quot;Web&quot;, &quot;Web&quot;, &quot;Web&quot;, &quot;Web&quot;, &quot;Web&quot;,… ## $ date_sent_to_company &lt;date&gt; 2019-09-24, 2019-10-25, 2019-11-08, 2019… ## $ company_response_to_consumer &lt;chr&gt; &quot;Closed with explanation&quot;, &quot;Closed with e… ## $ timely_response &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;,… ## $ consumer_disputed &lt;chr&gt; &quot;N/A&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;N/A&quot;,… ## $ complaint_id &lt;dbl&gt; 3384392, 3417821, 3433198, 3366475, 33853… 提取美元金額 complaints$consumer_complaint_narrative |&gt; str_extract_all(&quot;\\\\{\\\\$[0-9\\\\.]*\\\\}&quot;) |&gt; compact() |&gt; head() ## [[1]] ## [1] &quot;{$21.00}&quot; &quot;{$1.00}&quot; ## ## [[2]] ## [1] &quot;{$2300.00}&quot; ## ## [[3]] ## [1] &quot;{$200.00}&quot; &quot;{$5000.00}&quot; &quot;{$5000.00}&quot; &quot;{$770.00}&quot; &quot;{$800.00}&quot; ## [6] &quot;{$5000.00}&quot; ## ## [[4]] ## [1] &quot;{$15000.00}&quot; &quot;{$11000.00}&quot; &quot;{$420.00}&quot; &quot;{$15000.00}&quot; ## ## [[5]] ## [1] &quot;{$0.00}&quot; &quot;{$0.00}&quot; &quot;{$0.00}&quot; &quot;{$0.00}&quot; ## ## [[6]] ## [1] &quot;{$650.00}&quot; 二元分類：將產品分類轉化為二元類別（“Credit” 和 “Other”），便於分類建模。 set.seed(1234) complaints2class &lt;- complaints |&gt; mutate(product = factor(if_else( product == paste(&quot;Credit reporting, credit repair services,&quot;, &quot;or other personal consumer reports&quot;), &quot;Credit&quot;, &quot;Other&quot;)) ) 每 30 行選取一行，減少數據量，同時分割數據 dat= filter(complaints2class,row_number() %% 30 == 1) dat=dat[,c(2,6)] dat_split &lt;- initial_split(dat, strata = product) dat_train &lt;- training(dat_split) dat_test &lt;- testing(dat_split) dim(dat_train) ## [1] 2930 2 dim(dat_test) ## [1] 978 2 table(dat_train$product) ## ## Credit Other ## 1366 1564 table(dat_test$product) ## ## Credit Other ## 456 522 4.2 Step 2. Specifying step elements for Recipe 將 consumer_complaint_narrative（消費者投訴的文本描述）轉換為數值化的特徵，供模型（例如朴素貝葉斯、SVM 等）訓練使用。 dat_rec &lt;- recipe(product ~ consumer_complaint_narrative, data = dat_train) |&gt; step_tokenize(consumer_complaint_narrative) |&gt; # 分詞 step_stopwords(consumer_complaint_narrative, # 移除停用詞 stopword_source = &quot;snowball&quot;) |&gt; # &quot;snowball&quot;,&quot;smart&quot;,&quot;stopwords-iso&quot; step_tokenfilter(consumer_complaint_narrative, max_tokens = 1e3) |&gt; # 篩選 Token step_tfidf(consumer_complaint_narrative) # TF-IDF 轉換 4.3 Step 3. Build a workflow for Machine Learning 使用Workflow（工作流）整合，將數據預處理（Recipe）與模型訓練串聯。 myWorkFlow &lt;- workflow() |&gt; add_recipe(dat_rec) 4.4 Step 4. K-Fold Cross-Validation 使用交叉驗證評估模型性能 （K-Fold Cross-Validation）： 將數據分為 K 等分（folds）。 每次訓練時使用 K-1 個部分的數據，剩下的一部分用於測試。 重複 K 次，確保每個部分都被用作一次測試集。 最後計算 K 次的平均性能作為模型的表現指標。 set.seed(234) Kfolds_CV &lt;- vfold_cv(dat_train,v=10,repeats = 1) Kfolds_CV ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2637/293]&gt; Fold01 ## 2 &lt;split [2637/293]&gt; Fold02 ## 3 &lt;split [2637/293]&gt; Fold03 ## 4 &lt;split [2637/293]&gt; Fold04 ## 5 &lt;split [2637/293]&gt; Fold05 ## 6 &lt;split [2637/293]&gt; Fold06 ## 7 &lt;split [2637/293]&gt; Fold07 ## 8 &lt;split [2637/293]&gt; Fold08 ## 9 &lt;split [2637/293]&gt; Fold09 ## 10 &lt;split [2637/293]&gt; Fold10 4.5 Step 5. Models Selection 4.5.1 Model 1. Naive Bayes # 定義 Naive Bayes 模型 if (FALSE) { library(discrim) nb_spec &lt;- naive_Bayes() |&gt; set_mode(&quot;classification&quot;) |&gt; #模型用於分類 set_engine(&quot;naivebayes&quot;) nb_spec nb_fit &lt;- myWorkFlow |&gt; add_model(nb_spec) |&gt; # fit(data = dat_train) } nb_spec &lt;- naive_Bayes() |&gt; set_mode(&quot;classification&quot;) |&gt; set_engine(&quot;naivebayes&quot;) nb_wf &lt;- workflow() |&gt; #構建工作流 add_recipe(dat_rec) |&gt; #加入前面定義的文本預處理步驟（如分詞、TF-IDF） add_model(nb_spec) # 加入 Naive Bayes 模型 nb_wf ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: naive_Bayes() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_tokenize() ## • step_stopwords() ## • step_tokenfilter() ## • step_tfidf() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Naive Bayes Model Specification (classification) ## ## Computational engine: naivebayes # 使用交叉驗證訓練模型 nb_rs &lt;- fit_resamples( nb_wf, resample=Kfolds_CV, control = control_resamples(save_pred = TRUE) ) collect_metrics(nb_rs) # 收集指標 ## # A tibble: 3 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.686 10 0.0106 Preprocessor1_Model1 ## 2 brier_class binary 0.311 10 0.0105 Preprocessor1_Model1 ## 3 roc_auc binary 0.772 10 0.00817 Preprocessor1_Model1 accuracy（準確率）： 平均值：0.660（66%）。 表示模型在交叉驗證中，正確分類樣本的比例。 這表明 Naive Bayes 模型的準確率是中等水平，可能還有改進空間。 brier_class（Brier 分數）： 平均值：0.337。 測量模型預測的概率質量，值越小越好。 對於二元分類，0.337 表示模型的預測概率還算穩定，但還可以優化。 roc_auc（ROC 曲線下面積）： 平均值：0.763（76.3%）。 表示模型區分正類（Credit）和負類（Other）的能力，值越接近 1 越好。模型對正負類別的區分能力優於隨機猜測（0.5）。 collect_predictions(nb_rs) # 收集預測 ## # A tibble: 2,930 × 7 ## .pred_class .pred_Credit .pred_Other id .row product .config ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;chr&gt; ## 1 Credit 1.00e+ 0 1.05e-14 Fold01 16 Credit Preprocessor1_Mode… ## 2 Credit 1.00e+ 0 4.42e- 6 Fold01 31 Credit Preprocessor1_Mode… ## 3 Credit 1 e+ 0 6.23e-72 Fold01 33 Credit Preprocessor1_Mode… ## 4 Credit 1 e+ 0 3.94e-39 Fold01 41 Credit Preprocessor1_Mode… ## 5 Credit 1 e+ 0 2.60e-35 Fold01 42 Credit Preprocessor1_Mode… ## 6 Credit 1 e+ 0 2.32e-17 Fold01 47 Credit Preprocessor1_Mode… ## 7 Other 2.73e-16 1 e+ 0 Fold01 51 Credit Preprocessor1_Mode… ## 8 Credit 1 e+ 0 4.22e-52 Fold01 56 Credit Preprocessor1_Mode… ## 9 Credit 1 e+ 0 1.63e-29 Fold01 58 Credit Preprocessor1_Mode… ## 10 Credit 1.00e+ 0 3.35e-15 Fold01 74 Credit Preprocessor1_Mode… ## # ℹ 2,920 more rows 混淆矩陣 .metric .estimator .estimate accuracy binary 0.6860068 kap binary 0.3893372 f_meas binary 0.7330238 npv binary 0.8788235 ppv binary 0.6072115 precision binary 0.6072115 recall binary 0.9245974 sens binary 0.9245974 spec binary 0.4776215 \\(總體觀察\\) 模型偏好檢測正類（Credit）：高召回率（0.925）和低特異性（0.478）表明模型更傾向於將樣本歸類為正類，導致假陽性較多。 負類預測相對穩定：陰性預測值（0.879）較高，說明模型對負類（Other）的預測相對穩定。 平衡性能表現：F1 分數（0.733）表明模型在精確率和召回率之間有一定的平衡，但提升精確率可以進一步改進模型性能。 \\(視覺化\\) collect_predictions(nb_rs) |&gt; group_by(id) |&gt; roc_curve(truth = product, .pred_Credit) |&gt; autoplot() + labs( color = NULL, title = &quot;ROC curve for US Consumer Finance Complaints&quot;, subtitle = &quot;Each resample fold is shown in a different color&quot; ) conf_mat_resampled(nb_rs, tidy = FALSE) |&gt; autoplot(type = &quot;heatmap&quot;) 4.5.2 Model 2. The null model (baseline) null_classification &lt;- null_model() |&gt; set_engine(&quot;parsnip&quot;) |&gt; set_mode(&quot;classification&quot;) null_rs &lt;- workflow() |&gt; add_recipe(dat_rec) |&gt; add_model(null_classification) |&gt; fit_resamples( resample=Kfolds_CV, control = control_resamples(save_pred = TRUE) ) collect_metrics(null_rs) ## # A tibble: 3 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.534 10 0.00859 Preprocessor1_Model1 ## 2 brier_class binary 0.249 10 0.000563 Preprocessor1_Model1 ## 3 roc_auc binary 0.5 10 0 Preprocessor1_Model1 準確率為 53.4%，說明在無任何模型學習的情況下，僅猜測數量最多的類別即可達到此準確率。 Brier 分數為 0.249，僅僅基於類別比例猜測，誤差相對較大，優化空間大。 ROC AUC 僅為 0.5，表示基準模型沒有任何區分正負類別的能力。 4.5.3 Model 3. LASSO ## Using regularization helps us choose a simpler model that we expect ## to generalize better to new observations, and variable selection ## helps us identify which features to include in our model. lasso_spec &lt;- logistic_reg(penalty = 0.01, mixture = 1) |&gt; set_mode(&quot;classification&quot;) |&gt; set_engine(&quot;glmnet&quot;) lasso_spec ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.01 ## mixture = 1 ## ## Computational engine: glmnet lasso_wf &lt;- workflow() |&gt; add_recipe(dat_rec) |&gt; add_model(lasso_spec) lasso_wf ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: logistic_reg() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_tokenize() ## • step_stopwords() ## • step_tokenfilter() ## • step_tfidf() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.01 ## mixture = 1 ## ## Computational engine: glmnet set.seed(2020) lasso_rs &lt;- fit_resamples( lasso_wf, resample=Kfolds_CV, control = control_resamples(save_pred = TRUE) ) lasso_rs_metrics &lt;- collect_metrics(lasso_rs) lasso_rs_predictions &lt;- collect_predictions(lasso_rs) lasso_rs_metrics ## # A tibble: 3 × 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.854 10 0.00656 Preprocessor1_Model1 ## 2 brier_class binary 0.107 10 0.00346 Preprocessor1_Model1 ## 3 roc_auc binary 0.927 10 0.00493 Preprocessor1_Model1 準確率（Accuracy）：模型準確率達到 85.4%，相比基準模型的 53.4% 提升明顯。LASSO 模型能有效利用數據中的特徵進行分類。 Brier 分數（Brier Class）：0.107 ，表示模型預測的結果與真實標籤的差距很小。 ROC AUC：0.927，模型對正負類別的區分能力非常好。 視覺化結果 lasso_rs_predictions |&gt; group_by(id) |&gt; roc_curve(truth = product, .pred_Credit) |&gt; autoplot() + labs( color = NULL, title = &quot;ROC curve for US Consumer Finance Complaints&quot;, subtitle = &quot;Each resample fold is shown in a different color&quot; ) conf_mat_resampled(lasso_rs, tidy = FALSE) |&gt; autoplot(type = &quot;heatmap&quot;) \\(Tuning\\) LASSO 模型有一個控制「特徵篩選強度」的開關，叫 penalty（懲罰值）。 這個懲罰值決定模型要 多狠心 地「扔掉」不重要的特徵（例如多餘的詞語）。 調參的目的是 找到最適合的懲罰值，既能讓模型 準確率高，又不會過於複雜。 tune_lasso_spec &lt;- logistic_reg(penalty = tune(), mixture = 1) |&gt; set_mode(&quot;classification&quot;) |&gt; set_engine(&quot;glmnet&quot;) tune_wf &lt;- workflow() |&gt; add_recipe(dat_rec) |&gt; add_model(tune_lasso_spec) tune_rs &lt;- tune_grid( tune_wf, resample=Kfolds_CV, grid = grid_regular(penalty(), levels = 30), #定義 penalty 的搜索空間，從小到大生成 30 個均勻分布的值。 control = control_resamples(save_pred = TRUE) ) tune_rs ## # Tuning results ## # 10-fold cross-validation ## # A tibble: 10 × 5 ## splits id .metrics .notes .predictions ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [2637/293]&gt; Fold01 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 2 &lt;split [2637/293]&gt; Fold02 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 3 &lt;split [2637/293]&gt; Fold03 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 4 &lt;split [2637/293]&gt; Fold04 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 5 &lt;split [2637/293]&gt; Fold05 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 6 &lt;split [2637/293]&gt; Fold06 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 7 &lt;split [2637/293]&gt; Fold07 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 8 &lt;split [2637/293]&gt; Fold08 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 9 &lt;split [2637/293]&gt; Fold09 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; ## 10 &lt;split [2637/293]&gt; Fold10 &lt;tibble [90 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; collect_metrics(tune_rs) ## # A tibble: 90 × 7 ## penalty .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 e-10 accuracy binary 0.805 10 0.00713 Preprocessor1_Model01 ## 2 1 e-10 brier_class binary 0.182 10 0.00601 Preprocessor1_Model01 ## 3 1 e-10 roc_auc binary 0.861 10 0.00589 Preprocessor1_Model01 ## 4 2.21e-10 accuracy binary 0.805 10 0.00713 Preprocessor1_Model02 ## 5 2.21e-10 brier_class binary 0.182 10 0.00601 Preprocessor1_Model02 ## 6 2.21e-10 roc_auc binary 0.861 10 0.00589 Preprocessor1_Model02 ## 7 4.89e-10 accuracy binary 0.805 10 0.00713 Preprocessor1_Model03 ## 8 4.89e-10 brier_class binary 0.182 10 0.00601 Preprocessor1_Model03 ## 9 4.89e-10 roc_auc binary 0.861 10 0.00589 Preprocessor1_Model03 ## 10 1.08e- 9 accuracy binary 0.805 10 0.00713 Preprocessor1_Model04 ## # ℹ 80 more rows \\(視覺化\\) autoplot(tune_rs) + labs( title = &quot;Lasso model performance across regularization penalties&quot;, subtitle = &quot;Performance metrics can be used to identity the best penalty&quot; ) show_best(tune_rs, metric=&quot;roc_auc&quot;) ## # A tibble: 5 × 7 ## penalty .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.00853 roc_auc binary 0.927 10 0.00498 Preprocessor1_Model24 ## 2 0.0189 roc_auc binary 0.924 10 0.00532 Preprocessor1_Model25 ## 3 0.00386 roc_auc binary 0.918 10 0.00551 Preprocessor1_Model23 ## 4 0.00174 roc_auc binary 0.904 10 0.00625 Preprocessor1_Model22 ## 5 0.0418 roc_auc binary 0.894 10 0.00582 Preprocessor1_Model26 tune_rs_auc &lt;- show_best(tune_rs, metric=&quot;roc_auc&quot;) |&gt; pull(mean) |&gt; max() |&gt; round(3) chosen_auc &lt;- tune_rs |&gt; select_by_one_std_err(metric = &quot;roc_auc&quot;, -penalty) chosen_auc ## # A tibble: 1 × 2 ## penalty .config ## &lt;dbl&gt; &lt;chr&gt; ## 1 0.0189 Preprocessor1_Model25 4.5.4 Choosing the Best Model out-of tuned LASSOs final_lasso &lt;- finalize_workflow(tune_wf, chosen_auc) final_lasso ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: logistic_reg() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 4 Recipe Steps ## ## • step_tokenize() ## • step_stopwords() ## • step_tokenfilter() ## • step_tfidf() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0.018873918221351 ## mixture = 1 ## ## Computational engine: glmnet fitted_lasso &lt;- fit(final_lasso, dat_train) fitted_lasso |&gt; extract_fit_parsnip() |&gt; tidy() |&gt; mutate(term = str_remove(term, &quot;tfidf_consumer_complaint_narrative_&quot;)) |&gt; arrange(-estimate) ## # A tibble: 1,001 × 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 received 7.77 0.0189 ## 2 situation 6.45 0.0189 ## 3 money 6.37 0.0189 ## 4 debt 5.37 0.0189 ## 5 bank 4.65 0.0189 ## 6 funds 4.53 0.0189 ## 7 pay 4.47 0.0189 ## 8 statements 4.29 0.0189 ## 9 customer 4.27 0.0189 ## 10 interest 3.88 0.0189 ## # ℹ 991 more rows fitted_lasso |&gt; extract_fit_parsnip() |&gt; tidy() |&gt; mutate(term = str_remove(term, &quot;tfidf_consumer_complaint_narrative_&quot;)) |&gt; arrange(estimate) ## # A tibble: 1,001 × 3 ## term estimate penalty ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 requisite -21.1 0.0189 ## 2 equifax -11.7 0.0189 ## 3 reporting -10.9 0.0189 ## 4 experian -10.7 0.0189 ## 5 report -10.2 0.0189 ## 6 credit -9.69 0.0189 ## 7 reported -5.45 0.0189 ## 8 fcra -5.34 0.0189 ## 9 information -5.31 0.0189 ## 10 transunion -5.31 0.0189 ## # ℹ 991 more rows "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
